# .env.example
# This is a template for your environment variables.
# To use, copy this file to a new file named .env and fill in your actual values.
# The .env file should NEVER be committed to version control.

# --- Spark Application Configuration ---
# Set to 'prod' to use HDFS/cluster settings, or 'dev' for local Delta Lake execution.
SPARK_ENV=env

# Name of the Spark application to be displayed in the Spark UI.
SPARK_APP_NAME=football_etl

# The address of the Spark master. For Docker, this is the service name (e.g., 'spark://spark-master:7077').
SPARK_MASTER=spark://spark-master:7077

# Optional: Set the driver memory for local 'dev' mode.
# If not set, Spark's default will be used.
SPARK_DEV_DRIVER_MEMORY=4g


# --- HDFS Configuration ---
# The IP or hostname of your HDFS NameNode.
# Using this variable makes it easy to update the host in one place.
HDFS_NAMENODE_HOST=172.27.76.14

# The default location for Spark-managed tables in HDFS.
HDFS_WAREHOUSE_PATH=hdfs://${HDFS_NAMENODE_HOST}:9000/user/your_hdfs_username/spark-warehouse

# The location of the raw, unprocessed Bronze data (e.g., JSON files).
HDFS_BRONZE_BASE_PATH=hdfs://${HDFS_NAMENODE_HOST}:9000/user/your_hdfs_username/bronze_raw

# The location for the processed Bronze Delta tables.
HDFS_BRONZE_DELTA_PATH=hdfs://${HDFS_NAMENODE_HOST}:9000/user/your_hdfs_username/bronze

# The WebHDFS URL for accessing HDFS via HTTP (often used by other tools).
HDFS_URL=http://${HDFS_NAMENODE_HOST}:9870

# The username for interacting with HDFS.
HDFS_USER=your_hdfs_username


# --- Databricks Configuration ---
# The URL of your Databricks workspace (e.g., https://dbc-....cloud.databricks.com).
DATABRICKS_HOST=https://your-workspace-name.cloud.databricks.com

# Your Databricks Personal Access Token (PAT), starting with 'dapi...'.
DATABRICKS_TOKEN=dapixxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# The target Unity Catalog catalog for writing Gold tables.
DATABRICKS_CATALOG=workspace

# The target Unity Catalog schema for writing Gold tables.
DATABRICKS_SCHEMA=gold